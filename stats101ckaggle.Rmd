---
title: "Stats 101C Final Project"
author: "Edwin Chau"
date: "11/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Load the dataset and libraries
```{r}
library(readr)
library(glmnet)

# change user before loading data
user <- "Roger"

readtrain <- function() {
  if (user == "Edwin") {
    train <- read_csv("~/Stats 101C/train.csv")
  }
  if (user == "Roger") {
    train <- read_csv("~/Downloads/fall-2019-stats-101c/train.csv")
  }
  if (user == "Joanna") {
    train <- read.csv("Stats 101C/train.csv")
  }
  return(train)
}

readtest <- function() {
  if (user == "Edwin") {
    test <- read_csv("~/Stats 101C/test.csv")
  }
  if (user == "Roger") {
    test <- read_csv("~/Downloads/fall-2019-stats-101c/test.csv")
  }
  if (user == "Joanna") {
    test <- read.csv("Stats 101C/test.csv")
  }
  return(test)
}

```

# Edwin

### Format the data

After loading the data we want to dummy code the categorical variables and separate the y values from the predictors. To dummy code we can create a design matrix. However, this might not work for the test data as we don't have y values to write a formula with.

```{r include=FALSE}
train <- readtrain()
test <- readtest()

wins <- train$HTWins
#train <- cbind(train[c(1:8)], scale(train[-c(1:8)]))

dmat <- model.matrix(HTWins ~ ., data=train)
testmat <- model.matrix( ~., data=test)
```

### Lasso : Finding best lambda with CV

```{r}
# very long command, run at your own risk
lassolog <- cv.glmnet(dmat, wins, alpha=1, family='binomial')

grid <- 10^seq(10, -2, length=100)
bestlam <- lassolog$lambda.min
bestlam 
```

### Lasso Coefficients 

```{r}
lassomod <- glmnet(dmat, wins, alpha=1, family='binomial', lambda=grid)
lassocoef <- predict(lassomod, type='coefficients', s=bestlam)

## all nonzero predictors, 27 total
lassocoef[which(lassocoef > 0), ]
```

There seems to only be 36 "useful" predictors. We can cross reference with Ridge results.

### Lasso Predictions

```{r}
predictions <- as.vector(predict(lassomod, s=bestlam, newx=testmat, type="response"))
predictions <- ifelse(predictions > 0.5, "Yes", "No")

write.csv(data.frame(id = test$id, HTWins = predictions), file = "predictions.csv", row.names = F)
```


Lasso using lasso

Retrain a lasso model on the variables from previous lasso? Grasping at straws 
```{r}
lassopred <- model.matrix(HTWins ~ (VT.TA.dreb + VT.TA.ast + VT.TA.pts + VT.OTS.fgm + VT.OTS.ast + VT.OTS.stl +
                    VT.OTS.blk + VT.OTS.pts + VT.OTA.blk + VT.OS1.plmin + VT.OS2.plmin + VT.OS3.plmin + VT.OS3.dreb + VT.OS4.plmin +
                    VT.OS4.dreb + HT.TS.fgm + HT.TS.ast + HT.TS.stl + HT.TS.pts + HT.TA.blk + HT.OTA.dreb + HT.OTA.ast + HT.OTA.pts +
                    HT.S1.plmin + HT.S1.pts + HT.S1.stl + HT.S2.pts + HT.S2.stl + HT.S3.plmin + HT.S3.pts + HT.S3.stl + HT.S4.pts)^2, data = train)

lasso2 <- cv.glmnet(lassopred, wins, alpha=1, family="binomial")
bestlam2 <- lasso2$lambda.min
lassocoef2 <- predict(lasso2, type='coefficients', s=bestlam2)
lassocoef2[which(lassocoef2 > 0), ]

testmat <- model.matrix( ~ (VT.TA.dreb + VT.TA.ast + VT.TA.pts + VT.OTS.fgm + VT.OTS.ast + VT.OTS.stl +
                    VT.OTS.blk + VT.OTS.pts + VT.OTA.blk + VT.OS1.plmin + VT.OS2.plmin + VT.OS3.plmin + VT.OS3.dreb + VT.OS4.plmin +
                    VT.OS4.dreb + HT.TS.fgm + HT.TS.ast + HT.TS.stl + HT.TS.pts + HT.TA.blk + HT.OTA.dreb + HT.OTA.ast + HT.OTA.pts +
                    HT.S1.plmin + HT.S1.pts + HT.S1.stl + HT.S2.pts + HT.S2.stl + HT.S3.plmin + HT.S3.pts + HT.S3.stl + HT.S4.pts)^2, data = test)

pred2 <- as.vector(predict(lasso2, s=bestlam2, newx=testmat, type="response"))
pred2 <- ifelse(pred2 > 0.5, "Yes", "No")

write.csv(data.frame(id = test$id, HTWins = pred2), file = "lassointeract.csv", row.names = F)
```


Ridge over whole data set

This got the highest testing error, which kinda makes sense since it keeps all the variables no matter how insignificant. Maybe add interaction terms for even higher accuracy
```{r}
dmatint <- model.matrix(HTWins ~ .^2, data=train[,-c(1:2, 4:5)])
testmatint <- model.matrix(~ .^2, data=test)
testmat <- cbind(model.matrix(~., data=test), testmatint[ ,c("VT.TS.tpm:VT.TS.tpa", "VT.TS.fta:VT.TA.pf", "VT.TS.stl:VT.TA.to", "VT.TS.pts:HT.pmxU",
                       "VT.TA.fgm:VT.TA.fga", "VT.TA.fgm:VT.TA.pts", "VT.TA.tpm:VT.TA.tpa", "VT.OTS.tpm:VT.OTS.tpa", 
                       "VT.OTS.fta:VT.OTA.pf", "VT.OTS.stl:VT.OTA.to", "VT.OTS.pts:VT.pmxU", "VT.OTA.fgm:VT.OTA.pts", 
                       "VT.OTA.tpm:VT.OTA.tpa", "VT.S1.plmin:VT.S2.plmin", "VT.S1.plmin:VT.S3.plmin", "VT.S1.plmin:VT.S4.plmin", 
                       "VT.S1.pts:HT.OS1.fgm", "VT.S2.plmin:VT.S3.plmin", "VT.S2.pts:HT.OS2.fgm", "VT.S3.plmin:VT.S4.plmin", 
                       "VT.S3.pts:HT.OS3.fgm", "VT.S4.pts:HT.OS4.fgm", "VT.S5.pts:VT.S5.min", "VT.S5.pts:HT.OS5.fgm", "VT.S5.min:HT.OS5.fgm", 
                       "VT.OS1.plmin:VT.OS2.plmin", "VT.OS1.plmin:VT.OS3.plmin", "VT.OS1.plmin:VT.OS4.plmin", "VT.OS1.fgm:HT.S1.pts", 
                       "VT.OS2.plmin:VT.OS3.plmin", "VT.OS2.dreb:VT.OS2.oreb", "VT.OS2.fgm:HT.S2.pts", "VT.OS3.fgm:HT.S3.pts", 
                       "VT.OS4.fgm:HT.S4.pts", "VT.OS5.fgm:HT.S5.pts", "VT.OS5.fgm:HT.S5.min", "VT.pmxW:HT.pmxW", "HT.S5.pts:HT.S5.min", 
                       "HT.OS2.dreb:HT.OS2.oreb")])
bigridge <- cv.glmnet(cbind(dmat, dmatint[ ,c("VT.TS.tpm:VT.TS.tpa", "VT.TS.fta:VT.TA.pf", "VT.TS.stl:VT.TA.to", "VT.TS.pts:HT.pmxU",
                       "VT.TA.fgm:VT.TA.fga", "VT.TA.fgm:VT.TA.pts", "VT.TA.tpm:VT.TA.tpa", "VT.OTS.tpm:VT.OTS.tpa", 
                       "VT.OTS.fta:VT.OTA.pf", "VT.OTS.stl:VT.OTA.to", "VT.OTS.pts:VT.pmxU", "VT.OTA.fgm:VT.OTA.pts", 
                       "VT.OTA.tpm:VT.OTA.tpa", "VT.S1.plmin:VT.S2.plmin", "VT.S1.plmin:VT.S3.plmin", "VT.S1.plmin:VT.S4.plmin", 
                       "VT.S1.pts:HT.OS1.fgm", "VT.S2.plmin:VT.S3.plmin", "VT.S2.pts:HT.OS2.fgm", "VT.S3.plmin:VT.S4.plmin", 
                       "VT.S3.pts:HT.OS3.fgm", "VT.S4.pts:HT.OS4.fgm", "VT.S5.pts:VT.S5.min", "VT.S5.pts:HT.OS5.fgm", "VT.S5.min:HT.OS5.fgm", 
                       "VT.OS1.plmin:VT.OS2.plmin", "VT.OS1.plmin:VT.OS3.plmin", "VT.OS1.plmin:VT.OS4.plmin", "VT.OS1.fgm:HT.S1.pts", 
                       "VT.OS2.plmin:VT.OS3.plmin", "VT.OS2.dreb:VT.OS2.oreb", "VT.OS2.fgm:HT.S2.pts", "VT.OS3.fgm:HT.S3.pts", 
                       "VT.OS4.fgm:HT.S4.pts", "VT.OS5.fgm:HT.S5.pts", "VT.OS5.fgm:HT.S5.min", "VT.pmxW:HT.pmxW", "HT.S5.pts:HT.S5.min", 
                       "HT.OS2.dreb:HT.OS2.oreb")]), wins, alpha=0, family="binomial")

bestlam3 <- bigridge$lambda.min

ridgeintcoef <- predict(bigridge, type="coefficient", s=bestlam3)
pred3 <- as.vector(predict(bigridge, type="response", s=bestlam3, newx=testmat))
pred3 <- ifelse(pred3 > 0.5, "Yes", "No")
table(pred3)

write.csv(data.frame(id = test$id, HTWins = pred3), file = "ridge.csv", row.names = F)
```



### Backward fit 

```{r}
library(MASS)
train$HTWins = ifelse(train$HTWins == "Yes", 1, 0)
back_fit = glm(HTWins ~ VT.TA.dreb + VT.TA.ast + VT.TA.pts + VT.OTS.fgm + VT.OTS.ast + VT.OTS.stl +
                VT.OTS.blk + VT.OTS.pts + VT.OTA.blk + VT.OS1.plmin + VT.OS2.plmin + VT.OS3.plmin + VT.OS3.dreb + VT.OS4.plmin +
                VT.OS4.dreb + HT.TS.fgm + HT.TS.ast + HT.TS.stl + HT.TS.pts + HT.TA.blk + HT.OTA.dreb + HT.OTA.ast + HT.OTA.pts +
                HT.S1.plmin + HT.S1.pts + HT.S1.stl + HT.S2.pts + HT.S2.stl + HT.S3.plmin + HT.S3.pts + HT.S3.stl + HT.S4.pts, 
               data = train, family = "binomial")
back_fit = stepAIC(back_fit, trace = F, direction = "both")

back_fit = glm(HTWins ~ ., data = train, family = "binomial")
back_fit = stepAIC(back_fit, trace = F, direction = "backward")
summary(forward_fit)
```


### Undid the cumulative rest, lowered test error even further somehow
```{r}
train$HTcumRest <- c(train$HTcumRest[1],diff(train$HTcumRest))
train$VTcumRest <- c(train$VTcumRest[1],diff(train$VTcumRest))
test$HTcumRest <- c(test$HTcumRest[1],diff(test$HTcumRest))
test$VTcumRest <- c(test$VTcumRest[1],diff(test$VTcumRest))

train$HTWins = ifelse(train$HTWins == "Yes", 1, 0)
gam_fit = gam(HTWins ~ VTcumRest + HTcumRest + VT.TS.fgm + VT.TS.fga + 
    VT.TS.tpa + s(VT.TS.fta, df = 4) + s(VT.TS.pts, df = 7) + VT.TA.tpa + VT.TA.fta + 
    s(VT.OTA.dreb, df = 7) + VT.OTA.ast + s(VT.OTA.blk, df = 7) +
    VT.S1.pts + VT.S1.ast + VT.S2.ast + VT.S3.ast + VT.S4.ast +
    VT.S5.pts + VT.S5.min + VT.S5.stl + VT.OS1.plmin + 
    VT.OS1.dreb + VT.OS1.fgm + VT.OS2.plmin + 
    VT.OS2.dreb + VT.OS3.plmin + 
    VT.OS3.dreb + VT.OS4.dreb + VT.pmxU , family = "binomial", data = train)
summary(gam_fit)

pred <- predict(gam_fit, newdata=data.frame(test))
pred <- ifelse(pred > 0.5, "Yes", "No")

winners = data.frame(id = test$id, HTWins = pred)
write.csv(winners, file = "fixedrest.csv", row.names = F)

```

### Plot correlations to find interactions
```{r}
library(corrplot)
corrplot(cor(lassopred))
```


# Roger

## Training
```{r}
train <- readtrain()
```

## Testing
```{r}
test <- readtest()
```


## Forward Stepwise
```{r}
library(MASS)
forward_fit = glm(HTWins ~ ., data = train, family = "binomial")
forward_fit = stepAIC(forward_fit, trace = F, direction = "forward")
summary(forward_fit)

important = coef(forward_fit)
important[important > 0.1]

log_fit = glm(HTWins ~ VT.TS.fgm + VT.TS.fga + 
    VT.TS.tpa + VT.TS.fta + VT.TS.pts + VTcumRest:VT.TA.tpa + VT.TA.fta + 
    VT.OTA.dreb + VT.OTA.ast + VT.OTA.blk +
    VT.S1.pts + VT.S1.ast + VT.S2.ast + VT.S3.ast + VT.S4.ast +
    VT.S5.pts + VT.S5.min + VT.S5.stl + VT.OS1.plmin + 
    VT.OS1.dreb + VT.OS1.fgm + VT.OS2.plmin + 
    VT.OS2.dreb + VT.OS3.plmin + 
    VT.OS3.dreb + VT.OS4.dreb + VT.pmxU, family = "binomial", data = train )
summary(log_fit)

prop.table(table(winners$HTWins))
prop.table(table(train$HTWins))
```


##GAM's
```{r}
library(gam)
gam_fit = gam(HTWins ~ VTcumRest + HTcumRest + VT.TS.fgm + VT.TS.fga + 
    VT.TS.tpa + s(VT.TS.fta, df = 4) + s(VT.TS.pts, df = 7) + VT.TA.tpa + VT.TA.fta + 
    s(VT.OTA.dreb, df = 7) + VT.OTA.ast + s(VT.OTA.blk, df = 7) +
    VT.S1.pts + VT.S1.ast + VT.S2.ast + VT.S3.ast + VT.S4.ast +
    VT.S5.pts + VT.S5.min + VT.S5.stl + VT.OS1.plmin + 
    VT.OS1.dreb + VT.OS1.fgm + VT.OS2.plmin + 
    VT.OS2.dreb + VT.OS3.plmin + 
    VT.OS3.dreb + VT.OS4.dreb + VT.pmxU , family = "binomial", data = train )
summary(gam_fit)
```

## Testing Predictions
```{r}
test_prob = predict(object = gam_fit, newdata = test, type = "response")
test_pred = ifelse(test_prob > 0.5, "Yes", "No")

winners = data.frame(id = test$id, HTWins = test_pred)
sum(winners$HTWins != winners2$HTWins)
write.csv(winners, file = "predictions", row.names = F)

```

##Random Forest
```{r}
library(randomForest)
train$VT = as.factor(train$VT)
train$HT = as.factor(train$HT)
train$VTleague = as.factor(train$VTleague)
train$HTleague = as.factor(train$HTleague)

test$VT = as.factor(test$VT)
test$HT = as.factor(test$HT)
test$VTleague = as.factor(test$VTleague)
test$HTleague = as.factor(test$HTleague)

classify = randomForest(factor(HTWins) ~. -id - gameID - date, data = train, important = T, trees = 9000)


test_prob = predict(object = classify, newdata = test, type = "response")

winners = data.frame(id = test$id, HTWins = test_prob)

write.csv(winners, file = "predictions", row.names = F)

prop.table(table(winners$HTWins))
sum(winners$HTWins != winners2$HTWins)
importance(classify)
```

## Correlations
```{r}
coors = cor(train[c("VTcumRest", "HTcumRest", "VT.TS.fgm", "VT.TS.fga", "VT.TS.tpa", "VT.TS.fta",
            "VT.TS.pts", "VT.TA.tpa", "VT.TA.fta", "VT.OTA.dreb", "VT.OTA.ast", "VT.OTA.blk", "VT.S1.pts", "VT.S1.ast", "VT.S2.ast", "VT.S3.ast", "VT.S4.ast", "VT.S5.pts", "VT.S5.min", "VT.S5.stl", "VT.OS1.plmin", "VT.OS1.dreb", "VT.OS1.fgm", "VT.OS2.plmin", "VT.OS2.dreb", "VT.OS3.plmin", "VT.OS3.dreb", "VT.OS4.dreb", "VT.pmxU")])
library(corrplot)

corrplot(coors, method = "circle")
```



# Joanna
```{r}
library(readr)
library(MASS)
library(leaps)
test <- read.csv("Stats 101C/test.csv")
train <- read.csv("Stats 101C/train.csv")
train=train[,c(3,11:ncol(train))]
```

## t-test
```{r}
 index<-c()
for (i in 2:ncol(train))
{
  yes=train[train$HTWins=="Yes",i]
  no=train[train$HTWins=="No",i]
  pv<-t.test(yes,no)
 
  if(pv$p.value<0.001)
  {
    index=c(index,i)
  }
}
index
```

### train dataset with only significant variables
```{r}
train=train[,c(1,index)]
```

### correlation matrix
```{r}
cor=cor(train[-1])
```

### train dataset deleting high correlation variables
```{r}
train=train[,-c(4,5,13,14,22,25,26,33,34,35,36,44,105,114,128,136,137,147)]
```


## forward selection
```{r}
regfwd=regsubsets(HTWins~.,data=train,method="forward",nvmax=96)
sum=summary(regfwd)
par(mfrow=c(2,2))
plot(sum$rss,xlab="number of predictors",ylab="RSS",type="l")
plot(sum$cp,xlab="number of predictors",ylab="Cp",type="l")
plot(sum$bic,xlab="number of predictors",ylab="BIC",type="l")
plot(sum$adjr2,xlab="number of predictors",ylab="Adjusted R^2",type="l")
```

### smallest test MSE
```{r}
train$HTWins<- as.numeric(train$HTWins)
test.mat=model.matrix(HTWins~.,data=train)
val.errors=c()
for (i in 1:95)
{
  coefi=coef(regfwd,id=i)
  pred=test.mat[,names(coefi)]%*% coefi
  val.errors[i]=mean((train$HTWins-pred)^2)
}
which.min(val.errors)
```

### only # variables data set
```{r}
coef=coef(regfwd,78)
names=names(coef)[-1]
train_30=train[,c("HTWins",names)]
```

### split training set into train and test
```{r}
set.seed(1)
train_ind=sample(length(train$HTWins),length(train$HTWins)/2)
test_ind=-train_ind
train_1=train_30[train_ind,]
test_1=train_30[test_ind,]
```

### logistic
```{r}
train_1$HTWins<-as.factor(train_1$HTWins)
log<- glm(HTWins~.,data=train_1,family=binomial)
```

### checking accuracy
```{r}
set.seed(1)
glm.probs=predict(log,test_1,type="response")
glm.pred=rep("Yes",nrow(test_1))
glm.pred[glm.probs<0.5]="No"
test_1$HTWins=ifelse(test_1$HTWins=="1","No","Yes")
table(glm.pred,test_1$HTWins)
mean(glm.pred==test_1$HTWins)
```

### final csv
```{r}
test_prob = predict(object = log, newdata = test, type = "response")
test_pred = ifelse(test_prob > 0.5, "Yes", "No")

winners = data.frame(id = test$id, HTWins = test_pred)

write.csv(winners, file = "model_log", row.names = F)
```

